---
title: "Modelos ARIMA"
author: |
        | Santiago Bohorquez Correa
        |
        | Universidad EAFIT
        | Escuela de Economía y Finanzas
output:
  revealjs::revealjs_presentation:
    css: EAFIT.css
    highlight: pygments
    center: true
    transition: slide
    reveal_options:
      slideNumber: true
---


# Modelos Autoregresivos

# Modelo AR(1)

#
Ahora si podemos definir nuestro primer modelo lineal, el modelo Autoregresivo de orden 1 o AR(1)

\begin{equation}
    (1-\phi  L)x_t = \varepsilon_t
\end{equation}

Que también podemos escribir como $x_t = \phi x_{t-1} + \varepsilon_t$. El AR(1)  depende entonces solo de la variable de interés rezagada y el proceso de innovación. 


#

¿Bajo que supuestos es el modelo AR(1) estacionario (débil)? Para esto miremos cual es la media y la varianza de este modelo.

\begin{align}
    E(x_t) & = E(\phi x_{t-1} + \varepsilon_t) \\
   & =   \phi E(x_{t-1}) + E(\varepsilon_t)  \\
  & =  \phi E[ (\phi x_{t-2} + \varepsilon_{t-1})] +  E(\varepsilon_t) \text{ Repetimos el proceso} \\
   & =   \phi^2 E[ x_{t-2}] +  \phi E[ \varepsilon_{t-1}] +  E(\varepsilon_t) \\
                 & =   \phi^2 E[(\phi x_{t-3} + \varepsilon_{t-2})]  +  \phi E[ \varepsilon_{t-1}] +  E(\varepsilon_t)
\end{align}

#

Si seguimos iterando hacia atrás obtenemos 
\begin{align}
    E(x_t) & = \phi^n E[x_{t-n}] + \phi^{n-1} E[\varepsilon_{t-(n-1)}]  + \dots + \phi E[ \varepsilon_{t-1}] +  E(\varepsilon_t) \\
    \\
    E(x_t) & = \phi^n E[x_{t-n}] + 0 + \dots + 0 + 0 \\
    E(x_t) & = \phi^n E[x_{t-n}]
\end{align}    

Suponemos que $n \rightarrow \infty$ por lo tanto para que la media exista y no cambie a través del tiempo $|\phi | \leq 1$, y como vimos en el caso del paseo aleatorio, $|\phi | \neq 1$, así $|\phi | < 1$

#

Ahora hacemos lo mismo para la varianza, por facilidad de exposición asumimos que $E[x_1] = 0$:    

\begin{align}
Var(x_t) & = E[(x_t - E[x_t])^2] \\
         & = E[x_t^2] \\
  & = E[(\phi x_{t-1} + \varepsilon_t)^2]  \\
  & = E[(\phi x_{t-1})^2] + 2E[\phi x_{t-1}\varepsilon_t] + E[\varepsilon_t^2] \\
         & = \phi^2 E[x_{t-1}^2] + 0 + E[\varepsilon_t^2] 
\end{align}    

#

Si seguimos iterando hacia atrás obtenemos


\begin{align}
Var(x_t) & = \phi^2 E[(\phi x_{t-2} + \varepsilon_{t-1})^2] + E[\varepsilon_t^2] \\
         & = \phi^4 E[x_{t-2}^2] + \phi^2 E[\varepsilon_{t-1}^2] + E[\varepsilon_t^2] \\
\end{align}

#
Repitiendo este proceso, obtenemos

\begin{align}
Var(x_t) & = \phi^{2n} E[x_{t-n}^2] + \phi^{2(n-1)} E[\varepsilon_{t-n}^2 ] + \dots  \\
& + \phi^2 E[\varepsilon_{t-1}^2] + E[\varepsilon_t^2]
\end{align}    

#


Dado que mostramos que para ser estacionaria en media $|\phi| < 1$, 

\begin{align}
Var(x_t) & = \phi^{2(n-1)} E[\varepsilon_{t-n}^2 ] + \dots  + \phi^2 E[\varepsilon_{t-1}^2] + E[\varepsilon_t^2]
\end{align}

Que podemos escribir como $Var(x_t) = E[\alpha(L)\varepsilon_t]$. Ahora, si multiplicamos por $\phi^2$ obtenemos

\begin{align}
\phi^2E[\alpha(L)\varepsilon_t] & =  \phi^{2n} E[\varepsilon_{t-n}^2 ] + \dots  + \phi^4 E[\varepsilon_{t-1}^2] + \phi^2 E[\varepsilon_t^2]   
\end{align}

#

Reemplazamos por el valor de la esperanza,

\begin{align}
E[\alpha(L)\varepsilon_t] & =  \phi^{2(n-1)} \sigma^2 + \dots  + \phi^2 \sigma^2 + \sigma^2 \\
\phi^2E[\alpha(L)\varepsilon_t] & = \phi^{2n} \sigma^2 + \dots  + \phi^4 \sigma^2 + \phi^2 \sigma^2 
\end{align}

Y restamos

\begin{align}
E[\alpha(L)\varepsilon_t] - \phi^2E[\alpha(L)\varepsilon_t] & = \phi^{2(n-1)} \sigma^2 + \dots  + \phi^2 \sigma^2 + \sigma^2  \\
& -  \phi^{2n} \sigma^2 -  \dots  - \phi^4 \sigma^2 - \phi^2 \sigma^2 \\
(1 - \phi) E[\alpha(L)\varepsilon_t] & =  \sigma^2 -  \phi^{2n} \sigma^2 
\end{align}

#

dado que $|\phi | < 1$, finalmente obtenemos 

\begin{align}
    E[\alpha(L)\varepsilon_t] & = \frac{\sigma^2}{1 - \phi^2} \\
    Var(x_t) & = \frac{\sigma^2}{1 - \phi^2}
\end{align}

#

Finalmente, miremos la auto-covarianza para un proceso estacionario:

\begin{align}
    \gamma_j & = cov(x_t,x_{t-j}) \\
             & = E[(x_t x_{t-j}) - \mu_t\mu_{t-j}] \\
             & = E[x_t x_{t-j}]
\end{align}

Dado que $\mu_t=0$

#
    
Reemplazamos $x_t$ y obtenemos:

\begin{align}
    \gamma_j & = E[ (\phi x_{t-1} + \varepsilon_t ) x_{t-j}] \\
  & = E[ \phi x_{t-1} x_{t-j} + \varepsilon_t x_{t-j}] \\
   & = \phi E[x_{t-1} x_{t-j}] + E[\varepsilon_t x_{t-j}] \\
    & = \phi E[x_{t-1} x_{t-j}]
\end{align}

Dado que, como probamos anteriormente $E[\varepsilon_t x_{t-j}]=0$   


#
Como vimos previamente $\gamma_j = E[x_t x_{t-j}]$, por lo tanto     

\begin{equation}
    \gamma_j = \phi \gamma_{j-1}
\end{equation}

resolvemos entonces para $\gamma_o$, $\gamma_1$ y generalizamos para $\gamma_j$. Es fácil ver que $\gamma_0$ es $Var(x_t)$

#
Por lo tanto:

\begin{align}
    \gamma_0 & = \frac{\sigma^2}{1 - \phi^2} \\
    \gamma_1 & = \phi \frac{\sigma^2}{1 - \phi^2} \\
    \gamma_2 & = \phi^2 \frac{\sigma^2}{1 - \phi^2} \\
             & \vdots \\
    \gamma_j & = \phi^j \frac{\sigma^2}{1 - \phi^2}        
\end{align}
 
    
#

Estimamos la auto-correlación

\begin{align}
    \rho_0 & = \frac{\gamma_0}{\gamma_0}  = 1 \\
    \rho_1 & =  \frac{\phi \gamma_0}{\gamma_0}  = \phi \\
    \rho_2 & =  \frac{\phi \gamma_1}{\gamma_0}  = \phi^2 \\
             & \vdots  \\
    \rho_j & =  \frac{\phi \gamma_{j-1}}{\gamma_0}  = \phi^j       
\end{align}
 
    

# Modelo AR(p)

#

Definimos el modelo AR(p) como
     
  \begin{equation}
         x_t = \phi_1 x_{t-1} + \phi_2 x_{t-2} + \dots + \phi_p x_{t-p} + \varepsilon_t \label{eq:arp}
     \end{equation}
    
  o usando el operador de rezagos
    
\begin{equation}
       \phi(L)x_t = \varepsilon_t
  \end{equation}

El proceso es estacionario si las soluciones de $\lambda_i$ de la ecuación

\begin{equation}
    \lambda^p - \phi_1 \lambda^{p-1} - \dots - \phi_p = 0  
\end{equation}

caen dentro del circulo de unidad    

#

Asumiendo estacionariedad y realizando el mismo proceso que vimos para el proceso AR(1) y Ar(2) podemos obtener la media del proceso como:

\begin{equation}
\mu = \frac{E[\varepsilon_t]}{1- \phi_1 - \phi_2 - \dots - \phi_p}    
\end{equation}

Así en el caso sin constante $\mu = 0$


#
    
Ahora para estimar las auto-covarianzas usando la misma estrategia que utilizamos para el modelo AR(2).

\begin{align}
E[x_t x_{t-j}] & =  E[(\phi_1 x_{t-1} + \phi_2 x_{t-2} + \dots + \phi_p x_{t-p} +  \varepsilon_t) x_{t-j}] \\
E[x_t x_{t-j}] & = \phi_1 E[x_{t-1}x_{t-j}] + \phi_2 E[x_{t-2}x_{t-j}] + \dots + \phi_p E[x_{t-p} x_{t-j}] \\
& + E[\varepsilon_t x_{t-j}] \\
\gamma_j & = \phi_1 \gamma_{j-1} + \phi_2 \gamma_{j-2} + \dots + \phi_p \gamma_{j-p} + E[\varepsilon_t x_{t-j}]
\end{align}


#

Ahora podemos estimar la varianza, como $\gamma_0$

\begin{equation}
    \gamma_j  = \phi_1 \gamma_{1} + \phi_2 \gamma_{2} + \dots + \phi_p \gamma_{p} + \sigma^2
\end{equation}

Y para valores de $j>0$

\begin{equation}\label{gamma:arp}
    \gamma_j = \phi_1 \gamma_{j-1} + \phi_2 \gamma_{j-2} + \dots + \phi_p \gamma_{j-p} 
\end{equation}


#
    
Y para las auto-correlaciones dividimos la ecuación \ref{gamma:arp} por $\gamma_0$ y obtenemos

\begin{equation}
   \rho_j = \phi_1 \rho_{j-1} + \phi_2 \rho_{j-2} + \dots + \phi_p \rho_{j-p}  
\end{equation}
   
con esto podemos estimar las ecuaciones de Yule-Walker, 
 
 \begin{align}
    \rho_1  & = \phi_1  + \phi_2 \rho_1 + \dots + \phi_p \rho_{p-1}  \\
    \rho_2  & = \phi_1 \rho_1 + \phi_2  + \dots + \phi_p \rho_{p-2} \\
            &  \vdots \\
    \rho_p  & = \phi_1 \rho_{p-1} + \phi_2 \rho_{p-2} + \dots + \phi_p 
\end{align}

#

Y en forma matricial para los términos de $\phi$,

\begin{equation}
\begin{bmatrix}
 \phi_1 \\ 
\phi_2 \\
\vdots \\
\phi_p
\end{bmatrix}    
=
\begin{bmatrix}
 1 & \rho_1 & \dots & \rho_{p-1} \\ 
\rho_1 & 1 & \dots & \rho_{p-2} \\
\vdots & \vdots & \ddots & \vdots \\
\rho_{p-1} & \rho_{p-2} & \dots & 1 
\end{bmatrix}^{-1}  
\begin{bmatrix}
 \rho_1 \\ 
\rho_2 \\
\vdots \\
\rho_p
\end{bmatrix} 
\end{equation}

    


# Modelos de Medias Móviles

#

Ahora miramos  los modelos de medias móviles, empezamos por el modelo de medias móviles de orden 1, o MA(1).

Un modelo MA(1) se define como,

\begin{equation}
    x_t = \varepsilon_t - \theta \varepsilon_{t-1}
\end{equation}

usando el operador de rezagos $x_t = (1-\theta L) \varepsilon_{t}$

#

Ahora calculamos la esperanza de este proceso,

\begin{align}
    E[x_t] & = E[\varepsilon_t - \theta \varepsilon_{t-1}] \\
    E[x_t] & = E[\varepsilon_t] - \theta E[\varepsilon_{t-1}] \\
    E[x_t] & = 0
\end{align}

#

Y la varianza

\begin{align}
    \gamma_0 & = E[x_t x_t] \\
             &  = E[(\varepsilon_t - \theta \varepsilon_{t-1})^2] \\
  & = E[\varepsilon_t^2] - 2\theta E[\varepsilon_t \varepsilon_{t-1}] + \theta^2 E[\varepsilon_{t-1}^2] \\
  & = \sigma^2 + \theta^2 \sigma^2 \\
                & = \sigma^2 (1+\theta^2)
\end{align}
    

#

Ahora, estimamos la auto-covarianza $\gamma_j$ del proceso,

\begin{align}
    \gamma_j & = E[x_t x_{t-j}] \\
             & = E[(\varepsilon_t - \theta \varepsilon_{t-1})((\varepsilon_{t-j} - \theta \varepsilon_{t-j-1})] \\
     & = E[\varepsilon_t \varepsilon_{t-j}] - \theta E[ \varepsilon_{t-1} \varepsilon_{t-j}] - \theta E[ \varepsilon_{t} \varepsilon_{t-j-1}] + \theta^2 E[ \varepsilon_{t-1} \varepsilon_{t-j-1}]
\end{align}    

#

Si  $j = 1$ obtenemos,

\begin{align}
    \gamma_1     & = E[\varepsilon_t \varepsilon_{t-1}] - \theta E[ \varepsilon_{t-1} \varepsilon_{t-1}] - \theta E[ \varepsilon_{t} \varepsilon_{t-2}] + \theta^2 E[ \varepsilon_{t-1} \varepsilon_{t-2}] \\
    & =  0  - \theta \sigma^2 - 0 + 0 \\
                & =   - \theta \sigma^2
\end{align}    

si $j=2$,
\begin{align}
    \gamma_2     & = E[\varepsilon_t \varepsilon_{t-2}] - \theta E[ \varepsilon_{t-1} \varepsilon_{t-2}]   - \theta E[ \varepsilon_{t} \varepsilon_{t-3}] + \theta^2 E[ \varepsilon_{t-1} \varepsilon_{t-3}] \\
                 & = 0
\end{align} 

#

En general, para todo $j>1$, obtenemos


\begin{align}
    \gamma_j     & =  E[\varepsilon_t \varepsilon_{t-j}] - \theta E[ \varepsilon_{t-1} \varepsilon_{t-j}] - \theta E[ \varepsilon_{t} \varepsilon_{t-j-1}] + \theta^2 E[ \varepsilon_{t-1} \varepsilon_{t-j-1}] \\
                 & = 0
\end{align} 

#

Noten que para cualquier valor de $\theta$, el proceso es estacionario débil, ya que no se deben imponer restricciones sobre el parámetro para obtener la media, varianza o auto-covarianzas
    
#

Finalmente, estimamos las auto-correlaciones $\rho_i$,

\begin{align}
    \rho_1 & = \frac{\gamma_1}{\gamma_0} \\
           & = \frac{- \theta \sigma^2}{\sigma^2 (1+\theta^2)} \\
     & = \frac{- \theta }{ (1+\theta^2)}
\end{align}

y dado que $\gamma_j=0$ para todo $j>1$, esto implica que $\rho_j=0$ para todo $j>1$

#

Para un proceso AR(1) vimos que se cumplía la condición $|\rho| < 1$, sin embargo para procesos MA(1) si maximizamos el posible valor de $\rho_1$ encontramos

\begin{align}
\frac{\mathrm{d} \rho_1 }{\mathrm{d} \theta} & = \frac{-(1+\theta^2) + 2\theta^2}{(1+\theta^2)^2} = 0 \\
& -1 + \theta^2 = 0 \\
& \theta = \pm 1
\end{align}

reemplazando en $\rho_1$, obtenemos $| \rho_1 | \leq \frac{1}{2}$

#

Ahora, miremos la invertibilidad del proceso, para esto expresamos el proceso MA(1) en términos similares al AR(1), 

\begin{align}
  \varepsilon_t & = x_t + \theta \varepsilon_{t-1} \\
              & = x_t + \theta (x_{t-1} + \theta \varepsilon_{t-2}) \\
  & = x_t + \theta x_{t-1} + \theta^2 (x_{t-2} + \theta \varepsilon_{t-3})  \\
  &     \vdots \\
              & = x_t + \theta x_{t-1} + \theta^2 x_{t-2} + \dots + \theta^n \varepsilon_{t-n}
\end{align}

#

Si $| \theta | < 1$ el termino $\theta^n \varepsilon_{t-n} \rightarrow 0$ cuando $n \rightarrow \infty$, por lo cual bajo esta condición podemos expresar el proceso como

\begin{align}
  \varepsilon_t  & = x_t + \theta x_{t-1} + \theta^2 x_{t-2} + \dots + \theta^{n-1} x_{t-n-1} 
\end{align}

De esta forma pasamos de un modelo MA(1) a un AR($\infty$), a la condición $|\theta| < 1$ la llamamos la condición de invertibilidad


#

También, si $| \theta | < 1$, podemos escribir el proceso como

\begin{equation}
    \varepsilon_ = \frac{1}{1-\theta L} x_t
\end{equation}

#

Como se ve en la ecuación la condición de invertibilidad de un MA(1) es equivalente a la condición de estacionariedad de un modelo AR(1). Cabe recalcar de nuevo que el proceso MA(1) siempre es estacionario!, y la condición de invertibilidad es solo para poder pasar a un modelo AR($\infty$)

## Invertibilidad MA(1)

##

Noten que la invertibilidad tiene otras implicaciones, asumamos un valor $\theta^* > 1$ de forma tal que el proceso es no-invertible, la auto-correlación sería 
\begin{align}
  \rho_1 & = \frac{- \theta^* }{ (1+\theta^{**^2})} 
\end{align}

##

ahora, si tomamos $\theta^{**} = \frac{1}{\theta^*}$ este es un proceso invertible y la auto-correlación sería

\begin{align}
  \rho_1 & = \frac{- \theta^{**} }{ (1+\theta^{**^2})} \\
& = \frac{\frac{1}{\theta^*}}{1+ ( \frac{1}{\theta^*})^2} \\
    &  =  \frac{- \theta^*}{(1+\theta^{*^2})}
\end{align}

##

por lo tanto, se puede ver que para todo proceso no-invertible corresponde un proceso invertible con el mismo coeficiente de auto-correlación


## Invertibilidad AR(1)

##

Noten que podemos escribir el modelo AR(1) en términos de rezagos del proceso de innovación,

\begin{align}
    x_t & = \phi x_{t-1} + \varepsilon_t \\
        & = \phi (\phi x_{t-2} + \varepsilon_{t-1}) + \varepsilon_t \\
        & \vdots \\
        & = \phi^n x_{t-n} + \sum_{j=0}^n \phi^j  \varepsilon_{t-j}
\end{align}

Si, $|\phi| <1$ podemos escribir $x_t$ como un proceso $MA(\infty)$. En general, para los modelos AR estacionariedad implica invertibilidad  


# MA(q)

#

Ahora generalizamos para el caso con $q$ rezagos. Definimos el proceso MA(q) como,

\begin{equation}
    x_t = \varepsilon_t - \theta_1 \varepsilon_{t-1} - \theta_2  \varepsilon_{t-2} - \dots - \theta_q \varepsilon_{t-q}
\end{equation}

usando el operador de rezagos $x_t = (1-\theta_1 L - \theta_2 L^2 - \dots - \theta_q) \varepsilon_{t}$

#

Calculamos la esperanza de este proceso,

\begin{align}
    E[x_t] & = E[\varepsilon_t - \theta_1 \varepsilon_{t-1} - \theta_2 \varepsilon_{t-2} - \dots - \theta_q \varepsilon_{t-q}] \\
    E[x_t] & = E[\varepsilon_t] - \theta E[\varepsilon_{t-1}] - \theta_2 E[\varepsilon_{t-2}] - \dots - \theta_q E[\varepsilon_{t-q}] \\
    E[x_t] & = 0
\end{align}

#

Y la varianza

\begin{align}
    \gamma_0 & = E[x_t x_t] \\
             &  = E[(\varepsilon_t - \theta_1 \varepsilon_{t-1} - \theta_2 \varepsilon_{t-2} \dots - \theta_q \varepsilon_{t-q})^2] \\ \\
   & = \sigma^2 + \theta_1^2 \sigma^2 + \theta_2^2 \sigma^2 + \dots + \theta_q^2 \sigma^2 \\
                & = \sigma^2 (1+\theta_1^2 + \theta_2^2 + \dots + \theta_q^2)
\end{align}

#

Ahora, estimamos la auto-covarianza $\gamma_j$ del proceso,

\begin{align}
    \gamma_j & = E[x_t x_{t-j}] \\
             & = E[(\varepsilon_t - \theta_1 \varepsilon_{t-1} - \theta_2 \varepsilon_{t-2} - \dots - \theta_q \varepsilon_{t-q}) \\
            & (\varepsilon_{t-j} - \theta_1 \varepsilon_{t-j-1} - \theta_2 \varepsilon_{t-j-2} - \dots - \theta_q \varepsilon_{t-j-q})] \\
\end{align}    

#

Veamos para el caso de $\gamma_1$,

\begin{align}
     \gamma_1  & = E[(\varepsilon_t - \theta_1 \varepsilon_{t-1} - \theta_2 \varepsilon_{t-2} - \dots - \theta_q \varepsilon_{t-q}) \\
            & (\varepsilon_{t-1} - \theta_1 \varepsilon_{t-2}  - \theta_2 \varepsilon_{t-3} - \dots - \theta_q \varepsilon_{t-q-1})] \\
\end{align}
    
Sabemos que la esperanza de los términos $\varepsilon$ que no estén en el mismo periodo es 0. Entonces, en este caso obtendríamos,

\begin{align}
     \gamma_1  & = - \theta_1 \sigma^2 +  \theta_1\theta_2 \sigma^2 + \dots +   \theta_{q-1}\theta_q \sigma^2 \\
\end{align}

#

Ahora para $\gamma_2$,

\begin{align}
     \gamma_2  & = E[(\varepsilon_t - \theta_1 \varepsilon_{t-1} - \theta_2 \varepsilon_{t-2} - \dots - \theta_q \varepsilon_{t-q}) \\
            & (\varepsilon_{t-2} - \theta_1 \varepsilon_{t-3}  - \theta_2 \varepsilon_{t-4} - \dots - \theta_q \varepsilon_{t-q-2})] \\
\end{align}
    
Entonces, en este caso obtendríamos,

\begin{align}
     \gamma_2  & = - \theta_2 \sigma^2 +  \theta_1\theta_3 \sigma^2 + \dots +  \theta_{q-2}\theta_q \sigma^2 \\
\end{align}
 
#

Para el caso donde $1 \leq j \leq q$

\begin{align}
     \gamma_j  & = (-\theta_j + \theta_1 \theta_{j+1} + \theta_2 \theta_{j+2} + \dots + \theta_{q-j} \theta_q) \sigma^2
\end{align}
    
Y $\gamma_j=0$ para $j>q$


#

Estimamos las auto-correlaciones $\rho_j$,

\begin{align}
    \rho_j & = \frac{\gamma_j}{\gamma_0} \\
           & = \frac{-\theta_j + \theta_1 \theta_{j+1} + \theta_2 \theta_{j+2} + \dots + \theta_{q-j} \theta_q}{1+\theta_1^2 + \theta_2^2 + \dots + \theta_q^2}
 \end{align}
 
 para todo $1 \leq j \leq q$ y $\rho_j=o$ para todo $j>q$ 


#

Finalmente para invertibilidad del proceso MA(q) se debe cumplir que las soluciones de la ecuación, 

\begin{equation}
    \lambda^q - \theta_1 \lambda^{q-1} - \theta_2 \lambda^{q-2} - \dots - \theta_q = 0  
\end{equation}

caigan dentro del circulo de unidad. Este problema es equivalente al que realizamos para encontrar la estacionariedad del proceso AR(p). \frame{\tableofcontents[currentsection]}

# ARMA(p,q)

#

Definimos el proceso ARMA(p,q) como,

\begin{equation}
    x_t = \phi_1 x_{t-1} + \phi_2 x_{t-2} + \dots +  \phi_p x_{t-p} + \varepsilon_t - \theta_1 \varepsilon_{t-1} - \theta_2 \varepsilon_{t-2} - \dots - \theta_q\varepsilon_{t-q} 
\end{equation}

o usando el operado de rezagos $\phi(L)x_t = \theta(L)\varepsilon_t$

#

El proceso es estacionario si las soluciones para $\lambda$ de la ecuación $\lambda^p + \phi_1 \lambda^{p-1} + \dots + \phi_p = 0$ caen dentro del circulo de unidad y bajo este supuesto es fácil ver que para nuestro modelo 

\begin{align}
    E[x_t] & = 0
\end{align}

#

Y la varianza,

\begin{align}
    \gamma_0 & = E[x_t^2] \\
            & = E[(\phi_1 x_{t-1} + \phi_2 x_{t-2} + \dots +  \phi_p x_{t-p} + \varepsilon_t - \theta_1 \varepsilon_{t-1} - \\
            & \theta_2 \varepsilon_{t-2} - \dots - \theta_q\varepsilon_{t-q} )^2] \\ 
           & = \phi_1^2 E[x_{t-1}^2] + \phi_2^2 E[x_{t-2}^2] + \dots + E[\varepsilon_t^2] + \theta^2 E[\varepsilon_{t-1}^2] + \dots  \\
           & + 2 \phi E[x_{t-1}\varepsilon_t]  - 2 \theta \phi  E[x_{t-1}\varepsilon_{t-1}] - 2 \theta E[\varepsilon_t \varepsilon_{t-1}] + \dots \\
          & - 2 \phi_p \theta_q E[x_{t-p}\varepsilon_{t-q}]
\end{align}

#

Bajo el supuesto de estacionariedad, y sabiendo que $E[x_t\varepsilon_t] = \sigma^2$, obtenemos 

\begin{align}
    \gamma_0 &  = \phi_1^2 \gamma_0 + \phi_2^2 \gamma_0 + \dots + \sigma^2 + \theta_1^2 \sigma^2 + \dots \\
    & - 2 \theta_1 \phi_1 \sigma^2 - \dots - \phi_j \theta_j \sigma^2  \\
\gamma_0 & = \frac{(1+\theta_1^2 + \dots + \theta_q^2 - 2 \theta_1 \phi_1 - \dots - 2 \theta_i \phi_i) \sigma^2}{1-\phi_1^2 - \dots - \phi_p^2}
\end{align}


#

Para las auto-covarianzas tenemos,
\begin{align}
\gamma_j & = E[x_t x_{t-j}] \\
         & = \phi_1 E[x_{t-1} x_{t-j}] + \phi_2 E[x_{t-2} x_{t-j}] + \dots + E[\varepsilon_t  x_{t-j}] \\
         & - \theta_1 E[\varepsilon_{t-1} x_{t-j}] - \dots - \theta_q E[\varepsilon_{t-q} x_{t-j}]  \\
         & = \phi_1 \gamma_{j-1} + \dots + \phi_p \gamma_{j-p} + E[\varepsilon_t  x_{t-j}] \\
         & - \theta_1 E[\varepsilon_{t-1} x_{t-j}] - \dots - \theta_q E[\varepsilon_{t-q} x_{t-j}]
\end{align}

#

para $\gamma_j$ donde $1 \leq j \leq q$ sería,

\begin{align}
\gamma_j & = \phi_1 \gamma_{j-1} + \dots + \phi_p \gamma_{j-p}  \\
         & - \theta_1 E[\varepsilon_{t-1} x_{t-j}] - \dots - \theta_q E[\varepsilon_{t-q} x_{t-j}]
\end{align}

Y para $j>q$ obtenemos

\begin{align}
\gamma_j & = \phi_1 \gamma_{j-1} + \dots + \phi_p \gamma_{j-p} 
\end{align}

#

Por lo tanto la auto-correlación para $1 \leq j \leq q$ sería,

\begin{align}
    \rho_j & = \phi_1 \rho_{j-1} + \dots + \phi_p \rho_{j-p}  \\
         & + \frac{- \theta_1 E[\varepsilon_{t-1} x_{t-j}] - \dots - \theta_q E[\varepsilon_{t-q} x_{t-j}]}{\gamma_0}
\end{align}

y para $j>q$

\begin{align*}
    \rho_j & =\phi_1 \rho_{j-1} + \dots + \phi_p \rho_{j-p} 
\end{align*}

#

En los modelos ARMA(p,q) es conveniente factorizar las raíces de la parte autorregresiva y la parte de medias moviles, 

\begin{equation}
    \lambda^p + \phi_1 \lambda^{p-1} + \dots + \phi_p = 0
\end{equation}

y,

\begin{equation}
    \delta^q + \theta_1 \delta^{q-1} + \dots + \theta_q = 0
\end{equation}


#

Por lo tanto el ARMA(p,q) se puede expresar como,

\begin{equation}
    (1-\lambda_1 L) (1-\lambda_2 L) \dots (1-\lambda_p L) x_t = (1-\delta_1 L) (1-\delta_2 L) \dots (1-\delta_q L) \varepsilon_t
\end{equation}

Si existen dos raíces iguales, digamos $\lambda_1 = \delta_q$ podemos expresar el procesos como un ARMA(p-1,q-1),

\begin{equation}
     (1-\lambda_2 L) \dots (1-\lambda_p L) x_t = (1-\delta_1 L) (1-\delta_2 L) \dots (1-\delta_{q-1} L) \varepsilon_t
\end{equation}

#

Finalmente, para invertibilidad noten que el proceso se puede escribir como MA($\infty$) o AR($\infty$), lo podemos reescribir como

\begin{equation}
    x_t = \frac{\theta(L)}{\phi(L)} \varepsilon_t
\end{equation}



# ARIMA(p,d,q)

#

A un proceso integrado $x_t$ se le denomina como ARIMA(p,d,q), si aplicando primeras diferencias $d$ veces se obtiene un un proceso estacionario $w_t$ del tipo ARMA(p,q). Este se define como,

\begin{align}
    (1-\phi_1 L - \dots - \phi_p L^p) w_t & = (1-\theta_1 L - \dots - \theta_q L^q) \varepsilon_t \\
    (1-\phi_1 L - \dots - \phi_p L^p) (1 - L)^d x_t & = (1-\theta_1 L - \dots - \theta_q L^q) \varepsilon_t
\end{align}

O usando el operado de rezagos, $\phi(L)(1-L)^dx_t = \theta(L)\varepsilon_t$
